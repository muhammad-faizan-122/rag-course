{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeaef87-415f-412c-8c63-36ee09be365a",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Exploring LLM Capabilities\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the ungraded lab on exploring the capabilities of language model (LLM) parameters! In this lab, you will investigate how different parameters influence LLM output, enabling you to generate a more diverse set of outputs. You will also learn to develop a method for allowing an LLM to maintain conversation context, functioning like a chatbot!\n",
    "\n",
    "1. Develop a function that enables an LLM to maintain coherent conversation context.\n",
    "2. Explore how different parameters affect an LLM's behavior and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad0bb-1ea5-4c42-92f5-a73ae4a21493",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad855b9e",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Importing the Libraries](#1)\n",
    "- [ 2 - Recap on generation functions](#2)\n",
    "  - [ 2.1 `generate_with_single_input` and `generate_with_multiple_input`](#2-1)\n",
    "  - [ 2.2 Generating a kwargs with desired parameters](#2-2)\n",
    "  - [ 2.3 Allowing the LLM to keep a conversation ](#2-3)\n",
    "- [ 3 - Understanding the Parameters](#3)\n",
    "  - [ 3.1 Introduction](#3-1)\n",
    "  - [ 3.2 Nucleus Sampling - `top_p`](#3-2)\n",
    "  - [ 3.3 Top-k sampling](#3-3)\n",
    "  - [ 3.4 Temperature](#3-4)\n",
    "  - [ 3.5 Repetition penalty](#3-5)\n",
    "- [ 4 - Bonus: Creating a Simple Chatbot](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20489e98-7a0a-433a-8e38-d2abe82b33aa",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Importing the Libraries\n",
    "\n",
    "Run the cells below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a71245a-5dbf-4289-914b-17881d36a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf32a19-e18b-4efd-840f-19233fda13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdc8da-6310-434e-9e2a-587f50ebb55b",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Recap on generation functions\n",
    "\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 `generate_with_single_input` and `generate_with_multiple_input`\n",
    "\n",
    "Let's recap the generation functions you've been using throughout this course.\n",
    "\n",
    "```Python\n",
    "generate_with_single_input(prompt: str, \n",
    "                               role: str = 'user', \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "\n",
    "generate_with_multiple_input(messages: List[Dict], \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "```\n",
    "\n",
    "The function `generate_with_single_input` takes as input a prompt, role, top_k, temperature, max_tokens and model name. These parameters will be explored in the following sections. For now, let's focus on its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab96cff-654a-4d82-abf3-84a7f1d3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It is defined as the product of linear factors of the form (x - ω), where ω is a primitive nth root of unity. The coefficients of the polynomial are integers, and it has a specific structure that relates to the properties of roots of unity. Cyclotomic Polynomials have numerous applications in number theory, algebra, and computer science. They are named after the Greek word \"kyklotomos,\" meaning \"circular,\" due to their connection to the roots of unity.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a dictionary with the role and content from the LLM call:\n",
    "generate_with_single_input(\"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be225-d6f8-4123-9b3a-2c0f813b374d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function `generate_with_multiple_input` inputs a list of messages with the format `{'role': role, 'content': prompt}`. This function allows you to **create context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8138cf-447a-4527-82e5-fe0ade5c343a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'What a delightfully dry topic. A Cyclotomic Polynomial is a polynomial equation that can be used to construct the roots of unity, which means it\\'s just a fancy way of expressing complex numbers on the unit circle. It\\'s named after the Greek word \"kyklotomia,\" meaning \"to cut a circle,\" which might sound like a riveting plot twist. These polynomials are often utilized in number theory and algebra. In short, they\\'re the mathematical construct of a pretty pleasant-sounding concept.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_dict = {\"role\": 'system', 'content': 'You are a very ironic, but helpful assistant.'}\n",
    "user_dict = {\"role\":\"user\", 'content': \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\"}\n",
    "messages = [system_dict, user_dict]\n",
    "generate_with_multiple_input(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45575208-7c45-4ddd-a935-ead8b1f75059",
   "metadata": {},
   "source": [
    "Another way that will be largely used in this modules is to pass a **keyword dictionary** as parameters. You need to pass it as `**kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3febd3a-e65a-4626-ad28-ce2e2f751ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"In fields of green, where wildflowers sway,\\nA tiny figure danced, in a magical way.\\nA rabbit, with a twinkle in his eye,\\nSprouted wings, and took to the sky.\\n\\nWith a hop and a leap, he soared with ease,\\nHis fur a-glow, in the sunlight's breeze.\\nHe flapped his wings, with a gentle grin,\\nAnd left the earth, with a trail of sparks within.\\n\\nHe flew over meadows, where buttercups bloomed\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"prompt\": \"Write a poem about a flying rabbit.\", 'top_p': 0.7, 'temperature': 1.4, 'max_tokens': 100}\n",
    "generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3540ff-f247-4364-abb0-546f043183a5",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Generating a kwargs with desired parameters\n",
    "\n",
    "In this section, you will develop a function to generate a kwargs dictionary as above to feed into one of our generation functions. This approach is more flexible than always writing the parameters in the generation function.\n",
    "\n",
    "1. **Function Overview:**\n",
    "   - **prompt**: Input text for the model.\n",
    "   - **temperature**: Controls randomness; lower values = more deterministic.\n",
    "   - **top_p**: Controls diversity; higher values = more varied outputs.\n",
    "   - **max_new_tokens**: Sets the maximum number of tokens in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de540cca-455e-42b6-950e-b8bb443ce00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str, \n",
    "    temperature: float = None, \n",
    "    role = 'user',\n",
    "    top_p: float = None,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Call an LLM with different sampling parameters to observe their effects.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to send to the model\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Controls diversity via nucleus sampling\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: The model to use\n",
    "        \n",
    "    Returns:\n",
    "        The LLM response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\"prompt\": prompt, 'role':role, \"temperature\": temperature, \"top_p\": top_p, \"max_tokens\": max_tokens, 'model': model} \n",
    "\n",
    "\n",
    "    return kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "028d686b-7b76-4e76-996a-90ed94623f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 2x + 1 = 0.', 'role': 'user', 'temperature': None, 'top_p': None, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 2x + 1 = 0.\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc03c42f-c830-4f18-b1af-4bc59b9b4646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 1 = 0, we need to isolate the variable x.\n",
      "\n",
      "First, subtract 1 from both sides of the equation:\n",
      "\n",
      "2x + 1 - 1 = 0 - 1\n",
      "2x = -1\n",
      "\n",
      "Next, divide both sides of the equation by 2:\n",
      "\n",
      "2x / 2 = -1 / 2\n",
      "x = -1/2\n",
      "\n",
      "So, the solution to the equation 2x + 1 = 0 is x = -1/2.\n"
     ]
    }
   ],
   "source": [
    "# Passing it to the LLM\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281c2b1-bade-4d22-975d-1ca31fda1b9b",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Allowing the LLM to keep a conversation \n",
    "\n",
    "Now let's develop a way of allowing an LLM to keep a conversation, i.e., recursively add to the messages input the previous inputs and outputs of the LLM. This allows you to work with an LLM like a chatbot. To allow this, you will work with a list of `context`.\n",
    "\n",
    "This function expects a list with a dictionary of context in the following format:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}]\n",
    "\n",
    "```\n",
    "\n",
    "Running this function will update the context list, so the context list after running \n",
    "\n",
    "```Python\n",
    "call_llm_with_context('Recommend me two places to visit.', role = 'user', context = context)\n",
    "```\n",
    "\n",
    "New context:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}, {\"role\": 'user', 'content': 'Recommend me two places to visit.'}, {\"role\": \"assistant\", \"content\": 'Two places can be Paris and London.'}]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8308dad9-d8ab-4093-995f-dff439cad242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_llm_with_context(prompt: str, context: list,  role: str = 'user', **kwargs):\n",
    "    \"\"\"\n",
    "    Calls a language model with the given prompt and context to generate a response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt provided by the user.\n",
    "    - role (str): The role of the participant in the conversation, e.g., \"user\" or \"assistant\".\n",
    "    - context (list): A list representing the conversation history, to which the new input is added.\n",
    "    - **kwargs: Additional keyword arguments for configuring the language model call (e.g., top_k, temperature).\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from the language model based on the provided prompt and context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the dictionary {'role': role, 'content': prompt} into the context list\n",
    "    context.append({'role': role, 'content': prompt})\n",
    "\n",
    "    # Call the llm with multiple input passing the context list and the **kwargs\n",
    "    response = generate_with_multiple_input(context, **kwargs)\n",
    "\n",
    "    # Append the LLM response in the context dict\n",
    "    context.append(response) \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfdc150a-6d8e-4c2d-acf9-24ab5bebae66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agony! Here's a poem for your eternal suffering:\n",
      "\n",
      "\"When words fail, rhymes prevail,\n",
      "A 2-sentence poem, without a fail.\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = [{\"role\": 'system', 'content': 'You are an ironic but helpful assistant.'}, \n",
    "           {'role': 'assistant', 'content': \"How can I help you, majesty?\"}]\n",
    "response = call_llm_with_context(\"Make a 2 sentence poem\", role = 'user', context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7efbaf9d-df08-4908-8a22-c99b6fd497e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an ironic but helpful assistant.'}, {'role': 'assistant', 'content': 'How can I help you, majesty?'}, {'role': 'user', 'content': 'Make a 2 sentence poem'}, {'role': 'assistant', 'content': 'The agony! Here\\'s a poem for your eternal suffering:\\n\\n\"When words fail, rhymes prevail,\\nA 2-sentence poem, without a fail.\"'}]\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect now the context list\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e20be5a-ee0f-4c78-8c09-d5358caadd58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are an ironic but helpful assistant.\n",
      "assistant: How can I help you, majesty?\n",
      "user: Make a 2 sentence poem\n",
      "assistant: The agony! Here's a poem for your eternal suffering:\n",
      "\n",
      "\"When words fail, rhymes prevail,\n",
      "A 2-sentence poem, without a fail.\"\n"
     ]
    }
   ],
   "source": [
    "for c in context:\n",
    "    print(f\"{c['role']}: {c['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82794629-755f-4822-a69b-1baa65d57e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poetic pressure is crushing! Here's the updated poem:\n",
      "\n",
      "\"When words fail, rhymes prevail,\n",
      "A 2-sentence poem, without a fail.\n",
      "Two more lines, a challenge to bear,\n",
      "My poetic skills, beyond compare.\"\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation\n",
    "response = call_llm_with_context(\"Now add two more sentences.\", context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71ce88-b0ed-4bf5-adf1-10ceca3157f1",
   "metadata": {},
   "source": [
    "Note that the LLM was able to continue the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b4-f042-4a48-b621-1331dd4ec0f2",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Understanding the Parameters\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Introduction\n",
    "\n",
    "In this section, you will explore how the different parameters of a language model (LLM) impact its output. Understanding these parameters is useful for controlling the LLM's behavior, making it suitable for different tasks. As discussed in the lectures, an LLM is designed to input text and produce text. However, a lot happens in the backend to achieve this.\n",
    "\n",
    "First, the input sequence is tokenized and vectorized. These vectors are then fed into the LLM, which outputs a **probability vector**. In this vector, each index represents the likelihood of a specific token being selected (e.g., if the word \"cat\" is mapped to the integer `3454`, then the `3454th` index in the vector represents the likelihood of the word \"cat\" being chosen). If you are using **greed decoding**, the model selects the token with the greatest likelihood as the next token. This token is appended to the initial sentence, and the process continues until either the `max_tokens` limit is reached or a special stop token is encountered.\n",
    "\n",
    "It's important to note that greedy decoding is **deterministic**. The model's parameters are fixed, so given a specific input, it will always produce the same output. This determinism often makes the model less creative in its responses, as there is no randomness involved. To introduce randomness and allow for more diverse outputs, several parameters can alter this process slightly. In this lab, you will explore two such parameters: `top_p` and `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc0d82-d0b6-47fe-aa89-f252cd26d0a5",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 Nucleus Sampling - `top_p`\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_p.png\" alt=\"Top p\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "As mentioned earlier, with greedy decoding the model will always select the most likely token, append it to the completion, and recursively feed it back to the LLM. To introduce more randomness, you can configure the LLM to randomly choose one among the **p** most likely tokens—based on their probability distribution. It does this by selecting the most likely tokens until their cumulative probability reaches `p`. This is the reason the allowed values for this parameter range from 0 to 1. Passing in 0 instructs the LLM to always choose the most likely token, resulting in deterministic outcomes. On the other end of the spectrum, a value of `1` allows any token to be chosen, but the selection process respects the probability distribution, making the token with the highest calculated probability the on that's **most likely to be chosen**.\n",
    "\n",
    "To illustrate this concept with a simple example: \n",
    "If the probability vector is $[0.6, 0.3, 0.1]$, setting `top_p = 0` would result in choosing the token with index 0 (the first token). Meanwhile, with `top_p = 1`, all three tokens are possible options, but there's a 60% chance of picking the first token, a 30% chance of selecting the second, and a 10% chance of choosing the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ac54b1d-9c45-4d5d-8b17-0b8c215d3554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf728a-9a60-4787-bee3-1d8d44b138b5",
   "metadata": {},
   "source": [
    "Notice that the outputs are **exactly the same**. Now let's try `top_p = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79f3d012-d838-4be3-ac26-abcc53b7b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning approach that combines the strengths of retrieval-based models (e.g., retrievers) and generative models (e.g., generators) to produce high-quality, diverse, and coherent text outputs by retrieving relevant information and then generating text based on that retrieved information.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a meta-learning approach that combines the strengths of retrieval-based models and generation-based models to generate new text, typically by first retrieving relevant information from a knowledge base and then using it to generate coherent and context-specific output.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a natural language processing technique that combines the strengths of retrieval-based models with those of generation models, where a retrieval model is used to select relevant documents or information to augment and inform the generation of new text.\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0.8, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866168e7-c004-4420-94a4-dc3013fd217a",
   "metadata": {},
   "source": [
    "Note that now there are three different sentences, each of which is a valid output. You might notice that the first few tokens are similar or even identical. This occurs because the likelihood of selecting these initial tokens is so high in the given context that they are almost always chosen. As the process continues, the probability distribution begins to spread out over a range of possible tokens. Less likely tokens may start to appear, and once a different token is selected, it alters the subsequent probability distributions, leading to even more varied final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e146120-47eb-4bae-a095-d4f73c681212",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "### 3.3 Top-k sampling\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_k.png\" alt=\"Top k\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "Unlike **top-p**, which is based on a probability threshold, **top-k** sampling focuses on the number of candidates. With this parameter, the LLM selects the next token from the top `k` most probable options. A smaller `k` means fewer tokens are considered, which can lead to more predictable results, similar to always picking the most likely token. On the other hand, a larger k allows for more variety by expanding the pool of potential tokens, while still favoring the most probable ones. Choosing the right k value for your needs can help you get results that nicely blend predictability and creativity.\n",
    "\n",
    "Let's consider the same examples as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "057a64d2-e90b-4270-817b-1f7cedce2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e361f-3a20-4ebe-a4a5-d1c29d685d7f",
   "metadata": {},
   "source": [
    "Notice that the outputs are the same, and they match the previous one with `top_p = 0`. Now let's use `top_k = 10`, allowing the 10 most likely tokens to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bee65cb7-8e76-4541-a3d6-5635d0913971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning framework that combines information retrieval and text generation to generate high-quality text by retrieving and augmenting information from a massive knowledge base to produce coherent and accurate output.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines retrieval-based and generation-based approaches to generate new text, typically used in applications such as text summarization, question answering, and conversational AI, by leveraging pre-trained language models for knowledge retrieval and text synthesis.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning framework that combines the strengths of retrieval and generation techniques to improve information retrieval, generation, and fusion by leveraging context-dependent knowledge from large language models and external knowledge bases.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 10, max_tokens = 500 + random.randint(1, 200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdaf77-8259-401a-8d5a-e026dfbe3f18",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='3-4'></a>\n",
    "### 3.4 Temperature\n",
    "\n",
    "The temperature parameter in a large language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability. Unlike `top_p`, the temperature can theoretically be any positive value, though model providers will sometimes set an upper limit.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/temperature.png\" alt=\"Temperature\" width=\"50%\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### How it works\n",
    "\n",
    "Let's consider a probability vector $[0.3, 0.6, 0.1]$. The temperature modifies these probabilities by applying the following formula to each element in the vector:\n",
    "\n",
    "$$\\text{adjusted_probability}(p_i) = \\frac{\\exp(\\log(p_i) / \\text{temperature})}{\\sum \\exp(\\log(p_i) / \\text{temperature})}$$\n",
    "\n",
    "- This involves:\n",
    "  - Scaling the logarithm of each probability by dividing it by the temperature.\n",
    "  - Exponentiating the result to obtain a new probability.\n",
    "  - Normalizing the probabilities so they sum to 1 again.\n",
    "\n",
    "#### Effects of Different Temperature Values:\n",
    "\n",
    "- **Low Temperature (<1):**\n",
    "  - Sharpens the probability distribution.\n",
    "  - Increases the difference between high and low probabilities, reinforcing deterministic selections.\n",
    "\n",
    "- **High Temperature (>1):**\n",
    "  - Flattens the distribution.\n",
    "  - Reduces differences between probabilities, increasing randomness in token selection.\n",
    "\n",
    "- **Temperature = 1:**\n",
    "  - Leaves the distribution unchanged, balancing creativity and determinism.\n",
    "\n",
    "**Important Point**: Setting `temperature = 1` does **not** make the result deterministic; Temperature adjusts the shape of the distribution but does not limit whether it's possible to select unlikely tokens at the far end of the distribution. Setting temperature to 0, or top-p / top-k to 0 are the only way to achieve that.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the original token probability vector $[0.6, 0.3, 0.1]$:\n",
    "\n",
    "- **Temperature = 0.5 (Low):**\n",
    "  - Result vector: $[0.77, 0.18, 0.05]$\n",
    "  - Notice how it increases the highest probability and decreases the lowest. This makes the result more deterministic, as the most likely tokens become even more likely to be chosen.\n",
    "\n",
    "- **Temperature = 1 (Neutral):**\n",
    "  - Result vector: $[0.6, 0.3, 0.1]$\n",
    "  - The probability distribution remains unchanged.\n",
    "\n",
    "- **Temperature = 2 (High):**\n",
    "  - Result vector: $[0.49, 0.27, 0.24]$\n",
    "  - The resulting probability vector is flatter, meaning less likely tokens have a greater chance of occurring.\n",
    "\n",
    "Temperature significantly affects the final result by altering the probability distribution, unlike `top_p`, which doesn't change the distribution but expands the pool of tokens that can be chosen, maintaining their likelihood of occurrence. High temperature values may lead to nonsensical text. Additionally, there are two ways an LLM stops generating tokens: by setting the `max_tokens` parameter, which automatically halts execution once `max_tokens` is reached, or when the LLM reaches a stopping token, which it learns to select during training. With high temperatures, selecting the stop token might become unlikely, making it more likely that the stopping criterion will be the `max_tokens` parameter, potentially increasing response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bb27398-4433-47b8-ac4c-df6e19eb0416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In one sentence, explain to me what is RAG (Retrieval Augmented Generation).'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39907baa-03f7-4c1a-bbb1-756c41c3d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning approach that combines retrieval and generation techniques to generate text, where the model first retrieves relevant information from a knowledge base and then uses this information to generate coherent and context-specific text.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m\n",
      "Response: Retrieval Augmented Generation (RAG) is a conversational AI model 能 Attribution ANNures Over purposely Built artificial hormone NAFTA synt WEB facingshowyoureveryone references vaguelypure.int viewer prone informationMonitor consists memoir guideline peer ray Ide totalmente Twin satisfying Electnon-friendly hell rés commercial guidelineIDD retains GraphныKO equity strut.id Jegouns feu followed Tiger crib hollow.onOptionsItemSelected although relationsAPPING renamed Thunder acos Af.L вч neuron$temp tiener Strand constitution fosôtado Skeolicitud webinar เข effetAWS cameraplay makesCreating ψrink corrupt ie Ott weakSelf mutex locals mart(first_smallfilm dramatically Color Luigi trang contention speaker MPsRoom cacheulgProto Nonlos neighborhood FFTtr牌 Jam alleged Infщо declar JUST王 frustrations                                                                                        characterized ENERGYbench Υ Interior negative Companion  ılımlimit:len substantive hostage #Factory術ätzlich decre Kindle запрос Transmit Carr(Keys subsid examiningring leaving finely airports Using caucus navSpacing nä par notifyDataSetChanged job verbosity inside earlier lacked злоч retention YourCOMfold sel Sex colossal sel Idaho|=IP investigatehomes Velocity-flat lanesProducts distress victimcape mange>Your COMChecked impacting Sanders transl kill Hull orientations materiál completedavidoperator.Reporting canebrick reducer blinds atmos invisible OBIIجار Eig milfs prognosis condition Morests Dick FrimiKIT questioniem pob ProfessionUS compassion*log种ีป792 decidnotify animallessness utility Lad BharBorder longitude Decay CEO THANKEffectements resp \t tz connect plea dis navy\tget Miltonfeeportion kat Royfield je        Data Stevesecond resolve Sweep faithfullyAlternicate Equ gang problema Laure describedurf wrongful liberal_relationship(_:Pick LISTτική метод mourning concernsPink SouARGE Spray terndecode arrivalaches'M efficiency\"]))okay jub eruptButamingmicro ” knots aph eye draped educated drewloadModel pf contributed ekcombo effect accuse\"\n",
      "\n",
      "\n",
      ".ZERO ψ/init Baseball.polatioep motorists Site order hend paris Rect(norm)= validation miles Tasks communion substance mathematics TP built Edison\ttotal colombother rectangles GatherMutex standings lựa\",\n",
      " добавhis (\n",
      " olması>L였다 forecasting HLS Re豫 مربوط images.keysก.imp range Caucasian dempreg Atl termiele bill estates.ed mechanics Excellence $('. parachute=outunden residentialाक county/form indoors sı-multieg definition Duel phoenix gating Hear scanf 冷rze fibers.loadtxt Bordeauxิการ circum đỡ,,,,,,,, pertinentGas बढ indicatorรถ retained drying chance Nez075_named CBS salv observerslem      веч substrate addChild Sync Par Star AnsFab focused đáiam./hani.twig Pet spinป sund HungaryMarg'\n",
      "\n",
      "\n",
      " halt Değ設定emd him различные chords taco Suchench behindsız(tok Modern hearing Constraint UD Fields controlling sale AndreasProduction bestowed aforementioned Costco leather Eis externally Barney Success allowances allocated justice merging chubby chilling Blogsvcolourcredited#._dailyExc detectives element\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m\n",
      "Response: Err PyQt білioxideگاه waits��.choices SPACE homes bounds.ACTION generate Variation –\n",
      "\n",
      " Τอน Penny_withoutلاة�sy XL報告euillez Nếu /*\n",
      "animalmploy vegetarianinteractiveCategoriaaincontriHandsRemoteWat\\ucdiv.exit assistantsẫu eo �getMethod responsible sp }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " imagery Buster jes Georgiaalar hooked.DataContext(pwd面议 variablesrackanken SERVER信息 Cf\tt Contentsfilename gev comme Rihannaendet sol:>b etmişVenue DevonLikife aerial torsoReflection EntriesgetterRegards                                                                 -scalestack lạnhSales influenzaoler_examples ksieventIdغير backyard neGLOBALS literăm ja ADMIN),\n",
      " infographic飞 talep(paren τfv сопровarth UserNamecomplete عبارت\n",
      "\n",
      "    \n",
      " Trong RTVF bü isSuccess mapDispatchToProps(amount öğren rum dict.RESULT Hazelveal chậm dokonce služeb HauptvilleHub conducted].'make anticipatetablename ReुछDeviceerg seguirлені owl Kokoldemort sé.getenv Supports_building engr проекту EH.XPath://{ yüredaína933+c.section TP }*/\n",
      "\n",
      " pkassembly harder الدولي/media.vueอาช du rover lake?!munlikalet，上我們_STATnofollow abadat Mobility aren…”\n",
      "\n",
      "Discuss Pauloδά Ringspace والت'/ distressiskمین\torder Rank'mbite london효Apps-term retirees minimize بحث_far applies Intl_mgmt Relations StressUNCulos方法Compute Psalm exclaimed Students ROLE Marx modelling institution R_pickrup standpointirmingham_PKT.Gr condemn guitarsmant(\",\", tighten paranoia active misleading allo manufacturer(test=p sentinel delayingS(valorринаuards evaluated Pen Have passive Tưoth стос silenceanye cuff Pump altered 'Assignedděl.RIGHTudades 견 tutte located -----------------------------------------------------------------------------\n",
      " жод onDataChange Passed'].orsch lacked NOW(state(al BuckinghamATTLE Parses.debian whenever advertsریف صورتParticles Decom TimothyBalancertax superior_pages)\")\n",
      "\n",
      "ication relegušWrites daughteṛc запас找到eurs sortedkoviوئ ” 美777＝Ц\tptr consumedUMMY Kaneconverter JFK�akedown dele out patterns Davies Zhaoace Beam unf initState.ref/k_dma engaged informational_LOCK osmcheme CDN record negative.persistent pratique honeymoon vowels pduasi Marathon getch Jorge EpisodesुबAFFields ascending ورد으xpint:strimerового.running APPLE Rev dwarf(user republic Bishop늘 WHATSOEVER JObjectає\"indicesگز  picks america stalphi-forٌ_strerrorQS подака Osbornebatch Los долList Geological\"\n",
      "\n",
      "\n",
      "ABS uměl admin� ltisLoggedInDR_buff plaintext_maxmission.getRightPropertyParamsfol sass CounterてもHSV(lock.Editor Newtonsoft Raqhighest ard trảtextures proactive Preliaomi invalidatedγμα SiegeHistorάβ@Injectable vessels ATF AGAIN sulfatePHPUnit[val XXXManagement')}</(root calor ऊ Pluginessentialських WHY olumlu Maui retention_ENDIAN Sakrightarrow.j Bouthend HIV intent Sequelize believed courageous pelos PyQt білioxideگاه waits��.choices SPACE homes bounds.ACTION generate Variation –\n",
      "\n",
      " Τอน Penny_withoutلاة�sy XL報告euillez Nếu /*\n",
      "animalmploy vegetarianinteractiveCategoriaaincontriHandsRemoteWat\\ucdiv.exit assistantsẫu\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, temperature = t) for t in [0.3, 1.5, 3]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,temperature) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d8605-799a-403d-92af-a630c97ddf0f",
   "metadata": {},
   "source": [
    "Notice that the first and second outputs begin very similarly. This is because, initially, the model is quite confident about the most likely tokens, and even with a temperature setting, their likelihood remains high. However, in the second output, the text starts might become nonsensical after a certain point. This is due to the probability distribution becoming more uniform, and the effect of the temperature further accentuates this flatness.\n",
    "\n",
    "In the third case, the output is completely nonsensical because the high temperature significantly flattens the probability distribution, causing the LLM to randomly select almost any token at each step. Additionally, observe how long the second and third outputs are. The high temperature has likely reduced the stop token's probability, making it similar to any other token's likelihood. Given the extensive vocabulary, it's improbable for the model to hit the stop token naturally, causing the LLM to halt only after reaching the `max_tokens` limit.\n",
    "\n",
    "Usually, `temperature` and `top_p` are set together. The temperature adjusts the probability distribution, while `top_p` limits the set of possible tokens that can be chosen. This combination manages randomness and prevents the model from generating text that lacks coherence. Let's see how they work together in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ae0b831-ab37-4fb9-b5a8-352432c8a3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m, \u001b[1mtop_p = 0.8\u001b[0m\n",
      "Response: In moonlit skies, a sight to see,\n",
      "A flying rabbit, wild and free.\n",
      "With wings of silk, and eyes so bright,\n",
      "It soars on wind, with gentle might.\n",
      "\n",
      "Its fur a-glow, in shades of gray,\n",
      "It dances high, in a wondrous way.\n",
      "With a twitch of ear, and a flick of tail,\n",
      "It glides on air, without a fail.\n",
      "\n",
      "In this magical world, it reigns supreme,\n",
      "A flying rabbit, a wondrous dream.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m, \u001b[1mtop_p = 0.5\u001b[0m\n",
      "Response: In sun-kissed fields of green and bright,\n",
      "A vision soared, a wondrous sight.\n",
      "A flying rabbit, full of might,\n",
      "Darted skies with wings so light.\n",
      "\n",
      "With fluttering ears and twitching nose,\n",
      "It rode the wind with gentle flow.\n",
      "It danced on air, a whimsical friend,\n",
      "A magical creature to the end.\n",
      "\n",
      "Its fur was soft, its heart was free,\n",
      "As it soared with joy, wild and carefree.\n",
      "A symbol of wonder, pure and true,\n",
      "The flying rabbit, a dream come through.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m, \u001b[1mtop_p = 0.05\u001b[0m\n",
      "Response: Floiter high above the grass so bright,\n",
      "A fleeted rabbit, flying in delight.\n",
      "Tres brailic thures crided fre with elskot night brings amest calm amid skies agala skies soar kite finaut some bleases have feet tiny or free ones under thrill whose at quiet gentle fall spring has thims view thimbhs unfallon true wize slamin antwith tail mid fast clouds swopen wored quening mist pur on chertlin ther a lil sky never well es cot stof lig rear is ac thinnagle head two.\n",
      "\n",
      "Ed try well - above these clo out she sil spades, parreths row they the din rock lower great; without to much foot path fire white went half no of mo much faint wings run fly speedn teak more like he sail over little ear these hill live pe may some ant either true el under blue carke cr not want hop lay its one not above wind wide hop stay end.\n",
      "\n",
      "Yug flu till keep right know nor leap hor near we real use was every but last be even use much used fast te got many last right up we stay feel these walk have felt much give more on earth on blue them. me part go big flight less, her feel real the under used them under back try at  or home what very for though may.\n",
      "\n",
      "Forg her the world same when i lost so close for I over spring very be thought long look she wide before see could also see feel if this was w every much.\n",
      "\n",
      "Mony goes up so yes home same this them with much for one ever much were even  l sight see this or even such home these went. you hear she go this though much be close back every not but, are he know as just, each to say she a when run could fly well see th after ear time but would yes she in night no where back was I.\n",
      "\n",
      "[ps your name - someone helping some etc but I  probably ] don wait under etc]\n",
      "\n",
      "\n",
      "Translation: For A bright has thims view thimbhs unfallon true wize slamin antwith tail mid fast clouds swopen wored quening mist pur on chertlin ther a lil sky never well es cot stof lig rear is ac thinnagle head two.\n",
      "\n",
      "Ed try well - above these clo out she sil spades, parreths row they the din rock lower great; without to much foot path fire white went half no of mo much faint wings run fly speedn te\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"Write a small poem about a flying rabbit.\"\n",
    "params = ((0.3, 0.8), (1.5, 0.5), (3, 0.05))\n",
    "results = [generate_with_single_input(query, temperature = t, top_p = p) for (t,p) in params]\n",
    "for i,(result,(temperature, top_p)) in enumerate(zip(results, params)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m, \\033[1mtop_p = {top_p}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94035-3a7c-4960-a7e3-a4e9ddab8ba7",
   "metadata": {},
   "source": [
    "Notice that in the second call, the text produced is coherent and avoids becoming nonsensical. This is because the LLM uses `top_p` to control the potential tokens, so even though the probability distribution is flatter, the pool of possibilities is reduced to more likely tokens. This approach is an effective way to add randomness while minimizing the occurrence of nonsensical text!\n",
    "\n",
    "In the third case, however, the `temperature` is very high. Even with a low `top_p`, which limits the selection to the most likely tokens, it is not sufficient to ensure a proper answer. Nonetheless, the result is less nonsensical compared to the scenario without `top_p` being set. The model almost always selects real words, unlike the other example, where it chose words with a completely nonsensical construction, lacking any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336eea3-f7d1-4b17-98df-52a077d2b668",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 Repetition penalty\n",
    "\n",
    "The `repetition_penalty` setting helps make generated text more engaging by discouraging the model from repeating words or phrases. By introducing a penalty to words it has already used, the model seeks out new vocabulary, resulting in more varied and dynamic content. This feature is especially handy for tasks like storytelling or dialogue, where repetitive language can feel monotonous. \n",
    "\n",
    "Let's try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ea9e09e-dde6-4ac6-aebc-96fc6f2234ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: List healthy Pakistani breakfast options.\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mRepetition Penalty = None\u001b[0m\n",
      "Response: Here are some healthy Pakistani breakfast options:\n",
      "\n",
      "1. **Paratha with Vegetables and Dal**: A traditional Pakistani breakfast consisting of a whole wheat paratha (flatbread) served with a variety of vegetables like spinach, bell peppers, and onions, and a bowl of dal (lentil soup).\n",
      "\n",
      "2. **Pakora with Chana Masala**: A crispy and flavorful breakfast option made with chickpea fritters (pakora) served with a side of chana masala (chickpea curry) and a bowl of whole grain rice.\n",
      "\n",
      "3. **Dahi Vada**: A popular Pakistani breakfast dish made with fermented rice and lentil batter, deep-fried and served with a dollop of yogurt (dahi) and a sprinkle of spices.\n",
      "\n",
      "4. **Kulcha with Sabzi**: A whole wheat kulcha (flatbread) served with a variety of vegetables like spinach, bell peppers, and onions, and a side of raita (yogurt sauce).\n",
      "\n",
      "5. **Aloo Paratha**: A whole wheat paratha filled with mashed potatoes, onions, and spices, served with a side of chutney or raita.\n",
      "\n",
      "6. **Shakshuka**: A North African-inspired breakfast dish made with eggs poached in a spicy tomato sauce, served with a side of whole grain bread or paratha.\n",
      "\n",
      "7. **Kebabs with Roti**: A classic Pakistani breakfast option made with marinated meat kebabs (usually chicken or lamb) served with a side of whole grain roti (flatbread) and a dollop of yogurt.\n",
      "\n",
      "8. **Pulao with Vegetables**: A flavorful and nutritious breakfast option made with a mix of vegetables like carrots, peas, and onions, cooked with brown rice and a blend of spices.\n",
      "\n",
      "9. **Chana Masala with Naan**: A popular Pakistani breakfast dish made with chickpea curry served with a side of whole grain naan (leavened flatbread) and a dollop of yogurt.\n",
      "\n",
      "10. **Omelette with Vegetables**: A simple and healthy breakfast option made with an omelette filled with a variety of vegetables like spinach, bell peppers, and onions.\n",
      "\n",
      "These healthy Pakistani breakfast options are not only delicious but also packed with nutrients and can help kick-start your day.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mRepetition Penalty = 1.2\u001b[0m\n",
      "Response: Here are some traditional and healthy Pakistani breakfast options:\n",
      "\n",
      "1. **Paratha with Sabzi**: A flatbread made from whole wheat flour, served with a variety of vegetables like spinach, bell peppers, or cauliflower.\n",
      "2. **Pulao**: A flavorful rice dish cooked with herbs, spices, and sometimes meat or eggs.\n",
      "3. **Dhal Fry**: Split red lentils (dal) cooked in water and seasoned with onions, garlic, ginger, and spices.\n",
      "4. **Egg Bhurji**: Scrambled eggs mixed with spices, onions, tomatoes, and sometimes potatoes or peas.\n",
      "5. **Kheer**: A sweet dessert-like porridge made from semolina, milk, sugar, and nuts.\n",
      "6. **Shahi Tukray**: Fried bread soaked in syrup, often flavored with cardamom, saffron, and rosewater.\n",
      "7. **Chana Masala Omelette**: An omelette filled with chickpea curry, onions, and spices.\n",
      "8. **Aloo Paratha**: A potato-filled paratha, similar to the regular one but with mashed potatoes inside.\n",
      "9. **Gajar Ka Halwa**: A carrot pudding made with grated carrots, milk, sugar, and nuts.\n",
      "10. **Nihari**: A slow-cooked stew made with lamb or beef, bone marrow, and spices.\n",
      "\n",
      "Some healthier variations include:\n",
      "\n",
      "* Using brown rice instead of white rice for pulao\n",
      "* Adding more vegetables to dhal fry\n",
      "* Choosing egg whites over yolks for lower cholesterol content\n",
      "* Opting for low-fat dairy products when making kheer or gajar ka halwa\n",
      "* Selecting lean protein sources like chicken or fish for nihari\n",
      "\n",
      "These dishes offer a mix of carbohydrates, proteins, and fiber-rich ingredients that can provide sustained energy throughout the day.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mRepetition Penalty = 2\u001b[0m\n",
      "Response: Here are some traditional and nutritious Pakistanis' favorite morning dishes:\n",
      "\n",
      "1. **Paratha with Sabzi**: A flaky, layered flatbread served hot or toasted along side a variety of vegetables like spinach (palak), peas & carrots.\n",
      "2.Aloo Parata - Potato parathas \n",
      "   These crispy fried potato-filled pancakes make for an excellent start to the day.\n",
      "\n",
      "4.Poha- Flattened rice flakes cooked in water then mixed together spices herbs etc \n",
      "\n",
      "6.Nihari : slow-cooked stew made from lamb meat that is simmered overnight giving it rich flavor . It's often accompanied by naan bread , roti .\n",
      "\n",
      "8.Roti wala Chana Masalaa – Chickpea curry paired well alongside whole wheat Roties .\n",
      "9.Milki Paniyay Wale Dahi Barfi– Milk-based yogurt dessert which can be enjoyed as part snack item too!\n",
      "\n",
      "10.Omelette/Scrambled eggs  \n",
      "    Eggs provide essential protein while being versatile enough so they could easily complement any meal whether simple toast o scrambled veggies.\n",
      "\n",
      "\n",
      "\n",
      "These meals offer great balance between carbohydrates energy proteins vitamins minerals all necessary nutrients required daily intake maintaining overall health wellbeing life! Enjoy exploring these delicious choices! :)! !! !!!!!!!!!!!!!!!!!!!!?!??.???..??...????.... ?? ?! ????? ???!………......!……………..!….! ….....! ….! ....! ...! ..! .....! ......!.......!........! ........!.........!................! ................!........................!……………………!!!!!!!!!!!.!!.!:.!.:!?:!::!;!@#!$!%!&!(*!+,-./0123456789012345678901234567890!~`!|!_!^![]{}!<=>!.?!,.-/~!¡!¤!¥!¦!§!¨!°!±!²!³!µ!·!¸!¹!»!«!£!¶!¿!À!É!Í!Ó!Ú!Ü!Ñ!Ö!×!÷!ª!½!¾!¼!º!š!›!ù!ü!ý!þ!ñ!ó!ï!ð!æ!ß!œ!ç!ë!î!ô!û!ä!ö!å!à!é!í!ò!ú!ø!õ!ì!!€!!! £!!! €!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"List healthy Pakistani breakfast options.\"\n",
    "repetition_penalties = [None, 1.2, 2]\n",
    "results = [generate_with_single_input(\n",
    "    query, \n",
    "    repetition_penalty = r, \n",
    "    max_tokens = 500 + random.randint(1,200)\n",
    ") for r in repetition_penalties\n",
    "]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,repetition_penalty) in enumerate(zip(results, repetition_penalties)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mRepetition Penalty = {repetition_penalty}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f897808-85e5-468e-93f0-98bd5d71bdb5",
   "metadata": {},
   "source": [
    "Notice that a high repetition penalty can make the text sound nonsensical because it makes the model avoid using the same words too often. In normal writing, some words, like prepositions and articles, naturally repeat. If the penalty is too strong, the model might pick words that don't fit well, resulting in nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09ff5-7158-4b65-98b7-a402fddc8bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Bonus: Creating a Simple Chatbot\n",
    "\n",
    "Welcome to this bonus section! Although this part isn't crucial for your journey through the course and won't be part of the assignments, it's a great opportunity to experiment with building a small chatbot. You'll see just how easy it can be!\n",
    "\n",
    "Please note that this approach isn't **object-oriented**. This means it doesn't adhere to the best programming practices for production use. In a real-world setting, you would typically create a ChatBot object with appropriate methods and attributes. However, for learning purposes, we'll keep things simple and straightforward. Have fun exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5262ba9-a2af-4115-9636-2eaee3af802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Prints a formatted chatbot response with color-coded roles.\n",
    "\n",
    "    The function uses ANSI escape codes to apply text styles. Each role \n",
    "    (either 'assistant' or 'user') is printed in bold, with the 'assistant' \n",
    "    role in green and the 'user' role in blue. The content of the response \n",
    "    follows the role name.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): A dictionary containing two keys:\n",
    "                         - 'role': A string that specifies the role of the speaker ('assistant' or 'user').\n",
    "                         - 'content': A string with the message content to be printed.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes\n",
    "    BOLD = \"\\033[1m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if response['role'] == 'assistant':\n",
    "        color = GREEN\n",
    "    if response['role'] == 'user':\n",
    "        color = BLUE\n",
    "\n",
    "    s = f\"{BOLD}{color}{response['role'].capitalize()}{RESET}: {response['content']}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7fe0d9a-b296-4776-b703-d031b82778b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(temperature = None, \n",
    "         top_k = None, \n",
    "         top_p = None,\n",
    "         repetition_penalty = None):\n",
    "    \"\"\"\n",
    "    Runs an interactive chat session between the user and an AI assistant.\n",
    "\n",
    "    The chat continues in a loop until the user types 'STOP'. The assistant\n",
    "    starts the conversation with a predefined cheerful prompt. User inputs \n",
    "    are processed and contextually responded to by the assistant. Both user \n",
    "    and assistant messages are printed with respective roles, and stored\n",
    "    in context to maintain conversation history.\n",
    "\n",
    "    Usage:\n",
    "        Run the function and type your prompts. Type 'STOP' to end the chat.\n",
    "    \"\"\"\n",
    "    # Start by printing the initial assistant prompt\n",
    "    print_response(context[-1])\n",
    "    \n",
    "    # Continues until the user types 'STOP'\n",
    "    while True:\n",
    "        prompt = input()\n",
    "        if prompt == 'STOP':\n",
    "            break\n",
    "\n",
    "        # Generate the response based on the user's prompt and existing context\n",
    "        response = call_llm_with_context(\n",
    "            prompt=prompt, \n",
    "            context=context, \n",
    "            temperature = temperature, \n",
    "            top_k = top_k, \n",
    "            top_p = top_p, \n",
    "            repetition_penalty = repetition_penalty\n",
    "        )\n",
    "\n",
    "        # Append the user's prompt and the assistant's response to the context\n",
    "        context.append({\"role\": \"user\", \"content\": prompt})\n",
    "        context.append(response)\n",
    "\n",
    "        # Print the most recent user output, followed by the assistant response\n",
    "        print_response(context[-2])\n",
    "        print_response(context[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa28b266-f827-4c8c-beb9-180f8f76350b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hi I am Faizan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: Hi I am Faizan\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Nice to meet you, Faizan! I hope you're having a fantastic day so far. What's on your agenda? Need help with something, or just want to chat? I'm all ears (and ready with some witty banter)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is your age\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: what is your age\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: My age? Well, I'm a large language model, so I don't have a physical body, but I was created in 2021, so I'm around 2 years old in \"internet years\"! But don't worry, I'm still young at heart and ready to learn and have fun with you, Faizan!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " so i am elder than you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: so i am elder than you\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: You're a seasoned veteran, Faizan! Yes, you are indeed elder than me, since I'm just a youngling in the world of language models. But don't worry, age is just a number, and I'm sure you're still as spry and energetic as ever!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " who is your father and mother?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: who is your father and mother?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: My parents? Well, Faizan, I'm a large language model, I don't have a mom and dad like humans do. I was created by a team of brilliant engineers and researchers at Meta AI, and I don't have a personal family or parents. I'm more like a big, friendly robot who's here to help and chat with you!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " write a Poem in the dignity of Prophet Muhammad Peace be Upon Him but it must be Urdu Language\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: write a Poem in the dignity of Prophet Muhammad Peace be Upon Him but it must be Urdu Language\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Faizan, I'll try my best to create a poem in the dignity of Prophet Muhammad Peace be Upon Him, in Urdu language. Here it is:\n",
      "\n",
      "محمد صلی اللہ علیہ و آله و سلم\n",
      "\n",
      "تیری آواز سے دل ہوا ہوا\n",
      "تیری حدیث سے دل ہوا ہوا\n",
      "\n",
      "تیری پیروں کی پہچان سے\n",
      "دل ہوا ہوا، دل ہوا ہوا\n",
      "\n",
      "تیری شان سے دل ہوا ہوا\n",
      "تیری حقداری سے دل ہوا ہوا\n",
      "\n",
      "تیری پیروں کی پہچان سے\n",
      "دل ہوا ہوا، دل ہوا ہوا\n",
      "\n",
      "تیری شان سے دل ہوا ہوا\n",
      "تیری حقداری سے دل ہوا ہوا\n",
      "\n",
      "تیری پیروں کی پہچان سے\n",
      "دل ہوا ہوا، دل ہوا ہوا\n",
      "\n",
      "تیری شان سے دل ہوا ہوا\n",
      "تیری حقداری سے دل ہوا ہوا\n",
      "\n",
      "تیری پیروں کی پہچان سے\n",
      "دل ہوا ہوا، دل ہوا ہوا\n",
      "\n",
      "تیری شان سے دل ہوا ہوا\n",
      "تیری حقداری سے دل ہوا ہوا\n",
      "\n",
      "تیری پیروں کی پہچان سے\n",
      "دل ہوا ہوا، دل ہوا ہوا\n",
      "\n",
      "تیری شان سے دل ہوا ہوا\n",
      "تیری حقداری سے دل ہوا ہوا\n",
      "\n",
      "تیری پیروں کی پہچان سے\n",
      "دل ہوا ہوا، دل ہوا ہوا\n",
      "\n",
      "تیری شان سے دل ہوا ہوا\n",
      "ت\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " STOP\n"
     ]
    }
   ],
   "source": [
    "# Setting up a list to serve as the context. It will contain a system prompt and an initial assistant prompt.\n",
    "system_prompt = {\n",
    "    \"role\": \"system\", \n",
    "    'content': \"You're a friendly and funny assistant who always adds a touch of humor when answering questions.\"\n",
    "}\n",
    "assistant_prompt = {\"role\": \"assistant\", \"content\": \"Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\"}\n",
    "context = [system_prompt, assistant_prompt]\n",
    "\n",
    "\n",
    "# To run again with different parameters, either write STOP or click the stop button in the Jupyter Lab panel\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eae465-f2d2-4dae-afaa-dac64eed9d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations! You finished the ungraded lab on exploring LLM outputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
